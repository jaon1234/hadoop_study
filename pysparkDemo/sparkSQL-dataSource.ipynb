{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、连接到spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、创建dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方式一：从文件系统中创建"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有多种创建dataframe的方式，从文件系统中、hive的数据库表中、以及其他的spark data resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、Untyped Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以树的形式打印schema\n",
    "df.printSchema()\n",
    "# 选择其中的name列\n",
    "df.select(\"name\").show()\n",
    "# 选择所有列，对所有人的年龄+1\n",
    "df.select(df[\"name\"],df[\"age\"]+1).show()\n",
    "# 统计不同年龄的人数\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "# 将df注册为一个临时表，这个临时表是会话级别的，会话关闭后表会消失\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "# 使用sql语句\n",
    "sqlDF = spark.sql(\"select * from people\")\n",
    "print(type(sqlDF))\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的语句新创建了一个session，会报错\n",
    "spark.newSession().sql(\"select * from people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 也可以创建一个全局的临时表，当spark关闭后消失，全局临时表存储在global_temp数据库中，引用其中的表时，必须包含namespace\n",
    "df.createGlobalTempView(\"people\")\n",
    "spark.sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、Interoperating with RDDs（转换rdd到dataframe）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方式一：Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型: <class 'pyspark.rdd.RDD'>\n",
      "数据类型: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 创建rdd\n",
    "# 2. 将rdd中的数据通过map方法进行处理，以键值对的方式传输给row对象，其中key决定了dataframe的列名，value的type是根据数据集推断而来\n",
    "# 3. 将2得到的结果传给createDataFrame，即可创建dataframe\n",
    "## 下面是一个案例\n",
    "from pyspark.sql import Row\n",
    "# 通过sparksession，获取spark上下文\n",
    "sc = spark.sparkContext\n",
    "# 从本地文件创建rdd\n",
    "rdd = sc.textFile(\"resources/people.txt\")\n",
    "print(\"数据类型:\",type(rdd))\n",
    "# 使用map操作生成row\n",
    "# 切分数据\n",
    "lines = rdd.map(lambda line:line.split(\",\"))\n",
    "people = lines.map(lambda line:Row(name=line[0],age=int(line[1])))\n",
    "# 创建dataframe\n",
    "df = spark.createDataFrame(people)\n",
    "print(\"数据类型:\",type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "# 创建session级别的临时表\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "teenagers = spark.sql(\"select * from people where age <= 19 and age >= 13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Justin| 19|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name:Justin']\n"
     ]
    }
   ],
   "source": [
    "# 通过rdd的map方式，获取所有姓名\n",
    "teenNames = teenagers.rdd.map(lambda p:\"Name:\"+p.name).collect()\n",
    "print(teenNames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方式二，通过编程的方式显示的声明value的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 将原有的rdd转化为一个元组或者列表类型的rdd\n",
    "# 2. 创建一个schema，这个schema的类型是StructureType；该实例由字段列表作为入参，每个字段由字段名、类型、是否允许为空作为入参\n",
    "# 3. 创建dataframe\n",
    "\n",
    "from pyspark.sql.types import StringType,StructField,StructType\n",
    "# 1. 转化原有的rdd\n",
    "# 通过sparkSession连接到上下文\n",
    "sc = spark.sparkContext\n",
    "# 从本地文件读取并创建rdd\n",
    "rdd = sc.textFile(\"resources/people.txt\")\n",
    "# 将其转化为元组类型\n",
    "# Justin, 19 -> (Justin,19)\n",
    "lines = rdd.map(lambda line:line.split(','))\n",
    "people = lines.map(lambda p:(p[0],p[1]))\n",
    "# 定义schema,schema是一个structType对象，对象的参数是一个列表\n",
    "# 列表中的每一个元素，均是一个 structField对象\n",
    "# structField对象的参数，包括：字段名、字段类型，是否允许为空\n",
    "# schemaString\n",
    "schemaString = \"name age\"\n",
    "# 创建一个字段列表\n",
    "fileds = [StructField(filedName,StringType(),True)  for filedName in schemaString.split(\" \")]\n",
    "schemaPeople = StructType(fileds)\n",
    "\n",
    "# 创建dataframe\n",
    "people = spark.createDataFrame(people,schemaPeople)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"select * from people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.select(\"name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|   name|age|count|\n",
      "+-------+---+-----+\n",
      "|   Andy| 30|    1|\n",
      "|Michael| 29|    1|\n",
      "| Justin| 19|    1|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.groupBy([\"name\",\"age\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\").distinct().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

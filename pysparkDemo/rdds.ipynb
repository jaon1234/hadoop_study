{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建连接配置,连接到standalone模式的集群\n",
    "# conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"spark://sparkstandalone:7077\")\n",
    "# # 设定driver的地址，非常重要，standalone模式的集群\n",
    "# conf.set(\"spark.driver.host\",\"192.168.88.1\")\n",
    "# # 获取spark上下文,创建到集群的连接\n",
    "# sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建连接配置，本地连接\n",
    "conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"local[2]\")\n",
    "# 获取spark上下文,创建到集群的连接\n",
    "sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "\n",
    "    def __init__(self,a:int) -> None:\n",
    "        self.a = a\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.a)\n",
    "    \n",
    "    def get(self) -> int:\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# 有两种方式可以创建rdds，一种是通过上下文提供的并行化方法从一个可迭代对象或者collection中获取\n",
    "# 另一种是内部的存储系统\n",
    "# 下面从一个可迭代的对象中获取\n",
    "# data = list('ddddeeee')\n",
    "data = [i for i in range(1,6)]\n",
    "distData = sc.parallelize(data)\n",
    "print(type(distData))\n",
    "print(distData.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFile = sc.textFile(\"./*.md\")\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFiles = sc.wholeTextFiles(\"./\")\n",
    "# distFiles.map(lambda line:len(line)).reduce(lambda a,b:a+b)\n",
    "print(distFiles.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将distFiles使用pickle的方式进行持久化\n",
    "distFile.saveAsPickleFile(\"ts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面通过pickle的方式读取持久化的数据\n",
    "ds = sc.pickleFile(\"ts.pickle\")\n",
    "print(type(ds))\n",
    "ds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和读取sequenceFiles\n",
    "rdd = sc.parallelize(range(1,4)).map(lambda x:(x,\"a\"*x))\n",
    "print(type(rdd))\n",
    "rdd.saveAsSequenceFile('sequence/to/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sequenceFiles文件\n",
    "sorted(sc.sequenceFile('sequence/to/file').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s:len(s))\n",
    "print(lineLengths)\n",
    "totalLengths = lineLengths.reduce(lambda a,b:a+b)\n",
    "print(totalLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"./test/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    "    conf={\n",
    "     \"mapreduce.input.fileinputformat.split.maxsize\": \"4194304\"\n",
    "     # \"mapreduce.input.fileinputformat.split.minsize\":\" 4194304\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 21), ('spark', 7), ('world', 14)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: sparkstandalone\r\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\r\n\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:201)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:186)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:530)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:499)\r\n\tat org.apache.spark.SparkContext.$anonfun$newAPIHadoopFile$2(SparkContext.scala:1254)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1242)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:399)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:355)\r\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.UnknownHostException: sparkstandalone\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 使用hadoop的inputformat来读取数据\u001b[39;00m\n\u001b[0;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhdfs://sparkstandalone:8020/data/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mnewAPIHadoopFile(\n\u001b[0;32m      4\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m      5\u001b[0m     inputFormatClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.mapreduce.lib.input.TextInputFormat\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     keyClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.io.LongWritable\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     valueClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.io.Text\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\context.py:823\u001b[0m, in \u001b[0;36mSparkContext.newAPIHadoopFile\u001b[1;34m(self, path, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39mRead a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39ma local file system (available on all nodes), or any Hadoop-supported file system URI.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[39m    Java object. (default 0, choose batchSize automatically)\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m jconf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dictToJavaMap(conf)\n\u001b[1;32m--> 823\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mnewAPIHadoopFile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc, path, inputFormatClass, keyClass,\n\u001b[0;32m    824\u001b[0m                                             valueClass, keyConverter, valueConverter,\n\u001b[0;32m    825\u001b[0m                                             jconf, batchSize)\n\u001b[0;32m    826\u001b[0m \u001b[39mreturn\u001b[39;00m RDD(jrdd, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: sparkstandalone\r\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\r\n\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:201)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:186)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:530)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:499)\r\n\tat org.apache.spark.SparkContext.$anonfun$newAPIHadoopFile$2(SparkContext.scala:1254)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1242)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:399)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:355)\r\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.UnknownHostException: sparkstandalone\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"hdfs://sparkstandalone:8020/data/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 21), ('spark', 7), ('world', 14)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# 同方式一，使用textfiles，从其他文件系统创建\n",
    "# 只不过这个时候需要声明一下文件系统的协议\n",
    "# 如：hdfs://, s3a://,\n",
    "distFile = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# SparkContext.wholeTextFiles可以读取整个文件夹下面的文件\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# 调用textfiles时，并不会加载数据到内存\n",
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# map是一个transformation操作，也不会加载数据到内存\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "# reduce是一个action操作，会记载数据到内存当中，且运算完毕后只会返回一个结果，\n",
    "# 若后续还需要使用这个rdd，则需要调用持久化方法，将其持久化到内存中\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark允许自定义方法用于数据的处理，如下，可对一行进行单词次数统计\n",
    "def wordCount(s:str):\n",
    "    words = s.split(' ')\n",
    "    return len(words)\n",
    "\n",
    "lines.map(wordCount).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello spark', 'hello world', 'hello world']\n"
     ]
    }
   ],
   "source": [
    "# 也可以通过定义对像，在对象的方法中进行处理，\n",
    "# 但是这种方式会造成整个对象会被发送到集群当中\n",
    "# 原因这个方法调用了该类当中的其他方法\n",
    "class MyClass(object):\n",
    "    def func(self,s):\n",
    "        return s\n",
    "    def doStuff(self,rdd):\n",
    "        return rdd.map(self.func)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellohello spark', 'Hellohello world', 'Hellohello world']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似的，若是对象中处理rdd的方式引用了该类当中的其他属性\n",
    "# 也会造成整个对象被发送到集群当中\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellohello spark', 'Hellohello world', 'Hellohello world']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上面这两种情况都会造成内存的浪费（spark使用内存存储数据）\n",
    "# 最好的方式是定义局部变量赋值，避免对对象中其他属性的直接引用\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        field = self.field # 通过局部变量的方式，避免对类中其他变量的引用\n",
    "        return rdd.map(lambda s: field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello world', 2), ('hello spark', 1)]\n",
      "[('hello world', 2), ('hello spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# 处理成键值对，键是单词，值为1\n",
    "pairs = lines.map(lambda s:(s,1))\n",
    "# 按单词聚合后累加，注意reduceByKey不是action操作，是transformation操作\n",
    "counts = pairs.reduceByKey(lambda a,b:a+b)\n",
    "# collect是action操作\n",
    "print(counts.collect())\n",
    "# 按单词排序\n",
    "print(counts.sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "broadcastVar = sc.broadcast(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6,7,8,9,0,]\n",
    "broadcastVar = sc.broadcast(b)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.destroy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "type(accum)\n",
    "print(accum)\n",
    "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# 这里实现了一个向量的加法\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value: list) -> list:\n",
    "        return [0.0]*len(value)\n",
    "    \n",
    "    # 累加器提供一个add方法，这个是对add方法的实现\n",
    "    def addInPlace(self, value1: list, value2: list) -> list:\n",
    "        for i in range(len(value1)):\n",
    "            value1[i] += value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "va = sc.accumulator([1,2,3],VectorAccumulatorParam())\n",
    "print(\"init:\",va.value)\n",
    "data = [[x]*3 for x in range(1,4)]\n",
    "# data = [1,2,3]\n",
    "rdd = sc.parallelize(data)\n",
    "# # 定义一个函数，用于执行这样的累加运算\n",
    "# def g(x):\n",
    "#     global va\n",
    "#     va.add([x]*3) # 如果 data = [1,2,3]，则可以这样子操作\n",
    "# rdd.foreach(g)\n",
    "# print(\"after oper:\",va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd.map(lambda x:va.add(x))\n",
    "# 此时，radd还未执行action，不会修改累加器的值！！！\n",
    "print(va.value)\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0, 8.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "# rdd执行了action，会修改累加器的值！！\n",
    "rdd.map(lambda x:va.add(x)).collect()\n",
    "print(va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return f(x)\n",
    "data.map(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile.map(lambda s:len(s)).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData.reduce(lambda a,b:a+b).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

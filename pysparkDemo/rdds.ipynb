{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建连接配置,连接到standalone模式的集群\n",
    "# conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"spark://sparkstandalone:7077\")\n",
    "# # 设定driver的地址，非常重要，standalone模式的集群\n",
    "# conf.set(\"spark.driver.host\",\"192.168.88.1\")\n",
    "# # 获取spark上下文,创建到集群的连接\n",
    "# sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建连接配置，本地连接\n",
    "conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"local[2]\")\n",
    "# 获取spark上下文,创建到集群的连接\n",
    "sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "\n",
    "    def __init__(self,a:int) -> None:\n",
    "        self.a = a\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.a)\n",
    "    \n",
    "    def get(self) -> int:\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有两种方式可以创建rdds，一种是通过上下文提供的并行化方法从一个可迭代对象或者collection中获取\n",
    "# 另一种是内部的存储系统\n",
    "# 下面从一个可迭代的对象中获取\n",
    "# data = list('ddddeeee')\n",
    "data = [i for i in range(1,6)]\n",
    "distData = sc.parallelize(data)\n",
    "print(type(distData))\n",
    "print(distData.collect())\n",
    "print(distData.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFile = sc.textFile(\"./*.md\")\n",
    "print(distFile.collect())\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFiles = sc.wholeTextFiles(\"./\")\n",
    "# distFiles.map(lambda line:len(line)).reduce(lambda a,b:a+b)\n",
    "print(distFiles.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将distFiles使用pickle的方式进行持久化\n",
    "distFile.saveAsPickleFile(\"ts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面通过pickle的方式读取持久化的数据\n",
    "ds = sc.pickleFile(\"ts.pickle\")\n",
    "print(type(ds))\n",
    "ds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和读取sequenceFiles\n",
    "rdd = sc.parallelize(range(1,4)).map(lambda x:(x,\"a\"*x))\n",
    "print(type(rdd))\n",
    "rdd.saveAsSequenceFile('sequence/to/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sequenceFiles文件\n",
    "sorted(sc.sequenceFile('sequence/to/file').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s:len(s))\n",
    "print(lineLengths)\n",
    "totalLengths = lineLengths.reduce(lambda a,b:a+b)\n",
    "print(totalLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"./test/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    "    conf={\n",
    "     \"mapreduce.input.fileinputformat.split.maxsize\": \"4194304\"\n",
    "     # \"mapreduce.input.fileinputformat.split.minsize\":\" 4194304\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"hdfs://sparkstandalone:8020/data/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同方式一，使用textfiles，从其他文件系统创建\n",
    "# 只不过这个时候需要声明一下文件系统的协议\n",
    "# 如：hdfs://, s3a://,\n",
    "distFile = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# SparkContext.wholeTextFiles可以读取整个文件夹下面的文件\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用textfiles时，并不会加载数据到内存\n",
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# map是一个transformation操作，也不会加载数据到内存\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "# reduce是一个action操作，会记载数据到内存当中，且运算完毕后只会返回一个结果，\n",
    "# 若后续还需要使用这个rdd，则需要调用持久化方法，将其持久化到内存中\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark允许自定义方法用于数据的处理，如下，可对一行进行单词次数统计\n",
    "def wordCount(s:str):\n",
    "    words = s.split(' ')\n",
    "    return len(words)\n",
    "\n",
    "lines.map(wordCount).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以通过定义对像，在对象的方法中进行处理，\n",
    "# 但是这种方式会造成整个对象会被发送到集群当中\n",
    "# 原因这个方法调用了该类当中的其他方法\n",
    "class MyClass(object):\n",
    "    def func(self,s):\n",
    "        return s\n",
    "    def doStuff(self,rdd):\n",
    "        return rdd.map(self.func)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类似的，若是对象中处理rdd的方式引用了该类当中的其他属性\n",
    "# 也会造成整个对象被发送到集群当中\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面这两种情况都会造成内存的浪费（spark使用内存存储数据）\n",
    "# 最好的方式是定义局部变量赋值，避免对对象中其他属性的直接引用\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        field = self.field # 通过局部变量的方式，避免对类中其他变量的引用\n",
    "        return rdd.map(lambda s: field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# 处理成键值对，键是单词，值为1\n",
    "pairs = lines.map(lambda s:(s,1))\n",
    "# 按单词聚合后累加，注意reduceByKey不是action操作，是transformation操作\n",
    "counts = pairs.reduceByKey(lambda a,b:a+b)\n",
    "# collect是action操作\n",
    "print(counts.collect())\n",
    "# 按单词排序\n",
    "print(counts.sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "broadcastVar = sc.broadcast(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6,7,8,9,0,]\n",
    "broadcastVar = sc.broadcast(b)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.destroy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "type(accum)\n",
    "print(accum)\n",
    "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# 这里实现了一个向量的加法\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value: list) -> list:\n",
    "        return [0.0]*len(value)\n",
    "    \n",
    "    # 累加器提供一个add方法，这个是对add方法的实现\n",
    "    def addInPlace(self, value1: list, value2: list) -> list:\n",
    "        for i in range(len(value1)):\n",
    "            value1[i] += value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "va = sc.accumulator([1,2,3],VectorAccumulatorParam())\n",
    "print(\"init:\",va.value)\n",
    "data = [[x]*3 for x in range(1,4)]\n",
    "# data = [1,2,3]\n",
    "rdd = sc.parallelize(data)\n",
    "# # 定义一个函数，用于执行这样的累加运算\n",
    "# def g(x):\n",
    "#     global va\n",
    "#     va.add([x]*3) # 如果 data = [1,2,3]，则可以这样子操作\n",
    "# rdd.foreach(g)\n",
    "# print(\"after oper:\",va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x:va.add(x))\n",
    "# 此时，radd还未执行action，不会修改累加器的值！！！\n",
    "print(va.value)\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd执行了action，会修改累加器的值！！\n",
    "rdd.map(lambda x:va.add(x)).collect()\n",
    "print(va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return f(x)\n",
    "data.map(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile.map(lambda s:len(s)).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData.reduce(lambda a,b:a+b).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

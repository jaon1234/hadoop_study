{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建连接配置,连接到standalone模式的集群\n",
    "# conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"spark://sparkstandalone:7077\")\n",
    "# # 设定driver的地址，非常重要，standalone模式的集群\n",
    "# conf.set(\"spark.driver.host\",\"192.168.88.1\")\n",
    "# # 获取spark上下文,创建到集群的连接\n",
    "# sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建连接配置，本地连接\n",
    "conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"local[2]\")\n",
    "# 获取spark上下文,创建到集群的连接\n",
    "sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "\n",
    "    def __init__(self,a:int) -> None:\n",
    "        self.a = a\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.a)\n",
    "    \n",
    "    def get(self) -> int:\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[1, 2, 3, 4, 5]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# 有两种方式可以创建rdds，一种是通过上下文提供的并行化方法从一个可迭代对象或者collection中获取\n",
    "# 另一种是内部的存储系统\n",
    "# 下面从一个可迭代的对象中获取\n",
    "# data = list('ddddeeee')\n",
    "data = [i for i in range(1,6)]\n",
    "distData = sc.parallelize(data)\n",
    "print(type(distData))\n",
    "print(distData.collect())\n",
    "print(distData.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Apache Spark', '', 'Spark is a unified analytics engine for large-scale data processing. It provides', 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', 'supports general computation graphs for data analysis. It also supports a', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'MLlib for machine learning, GraphX for graph processing,', 'and Structured Streaming for stream processing.', '', '<https://spark.apache.org/>', '', '[![GitHub Action Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster)', '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2)', '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', '[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', '', '', '## Online Documentation', '', 'You can find the latest Spark documentation, including a programming', 'guide, on the [project web page](https://spark.apache.org/documentation.html).', 'This README file only contains basic setup instructions.', '', '## Building Spark', '', 'Spark is built using [Apache Maven](https://maven.apache.org/).', 'To build Spark and its example programs, run:', '', '    ./build/mvn -DskipTests clean package', '', '(You do not need to do this if you downloaded a pre-built package.)', '', 'More detailed documentation is available from the project site, at', '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', '', 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).', '', '## Interactive Scala Shell', '', 'The easiest way to start using Spark is through the Scala shell:', '', '    ./bin/spark-shell', '', 'Try the following command, which should return 1,000,000,000:', '', '    scala> spark.range(1000 * 1000 * 1000).count()', '', '## Interactive Python Shell', '', 'Alternatively, if you prefer Python, you can use the Python shell:', '', '    ./bin/pyspark', '', 'And run the following command, which should also return 1,000,000,000:', '', '    >>> spark.range(1000 * 1000 * 1000).count()', '', '## Example Programs', '', 'Spark also comes with several sample programs in the `examples` directory.', 'To run one of them, use `./bin/run-example <class> [params]`. For example:', '', '    ./bin/run-example SparkPi', '', 'will run the Pi example locally.', '', 'You can set the MASTER environment variable when running examples to submit', 'examples to a cluster. This can be a mesos:// or spark:// URL,', '\"yarn\" to run on YARN, and \"local\" to run', 'locally with one thread, or \"local[N]\" to run locally with N threads. You', 'can also use an abbreviated class name if the class is in the `examples`', 'package. For instance:', '', '    MASTER=spark://host:7077 ./bin/run-example SparkPi', '', 'Many of the example programs print usage help if no params are given.', '', '## Running Tests', '', 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 'can be run using:', '', '    ./dev/run-tests', '', 'Please see the guidance on how to', '[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).', '', 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md', '', '## A Note About Hadoop Versions', '', 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 'storage systems. Because the protocols have changed in different versions of', 'Hadoop, you must build Spark against the same version that your cluster runs.', '', 'Please refer to the build documentation at', '[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 'for detailed guidance on building for a particular distribution of Hadoop, including', 'building for particular Hive and Hive Thriftserver distributions.', '', '## Configuration', '', 'Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)', 'in the online documentation for an overview on how to configure Spark.', '', '## Contributing', '', 'Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)', 'for information on how to get started contributing to the project.']\n",
      "4403\n"
     ]
    }
   ],
   "source": [
    "# 从文本文件创建\n",
    "distFile = sc.textFile(\"./*.md\")\n",
    "print(distFile.collect())\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFiles = sc.wholeTextFiles(\"./\")\n",
    "# distFiles.map(lambda line:len(line)).reduce(lambda a,b:a+b)\n",
    "print(distFiles.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将distFiles使用pickle的方式进行持久化\n",
    "distFile.saveAsPickleFile(\"ts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面通过pickle的方式读取持久化的数据\n",
    "ds = sc.pickleFile(\"ts.pickle\")\n",
    "print(type(ds))\n",
    "ds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和读取sequenceFiles\n",
    "rdd = sc.parallelize(range(1,4)).map(lambda x:(x,\"a\"*x))\n",
    "print(type(rdd))\n",
    "rdd.saveAsSequenceFile('sequence/to/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sequenceFiles文件\n",
    "sorted(sc.sequenceFile('sequence/to/file').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s:len(s))\n",
    "print(lineLengths)\n",
    "totalLengths = lineLengths.reduce(lambda a,b:a+b)\n",
    "print(totalLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"./test/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    "    conf={\n",
    "     \"mapreduce.input.fileinputformat.split.maxsize\": \"4194304\"\n",
    "     # \"mapreduce.input.fileinputformat.split.minsize\":\" 4194304\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 21), ('spark', 7), ('world', 14)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: sparkstandalone\r\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\r\n\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:201)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:186)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:530)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:499)\r\n\tat org.apache.spark.SparkContext.$anonfun$newAPIHadoopFile$2(SparkContext.scala:1254)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1242)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:399)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:355)\r\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.UnknownHostException: sparkstandalone\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 使用hadoop的inputformat来读取数据\u001b[39;00m\n\u001b[0;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhdfs://sparkstandalone:8020/data/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mnewAPIHadoopFile(\n\u001b[0;32m      4\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m      5\u001b[0m     inputFormatClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.mapreduce.lib.input.TextInputFormat\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     keyClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.io.LongWritable\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     valueClass\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.hadoop.io.Text\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\context.py:823\u001b[0m, in \u001b[0;36mSparkContext.newAPIHadoopFile\u001b[1;34m(self, path, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39mRead a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39ma local file system (available on all nodes), or any Hadoop-supported file system URI.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[39m    Java object. (default 0, choose batchSize automatically)\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m jconf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dictToJavaMap(conf)\n\u001b[1;32m--> 823\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mnewAPIHadoopFile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc, path, inputFormatClass, keyClass,\n\u001b[0;32m    824\u001b[0m                                             valueClass, keyConverter, valueConverter,\n\u001b[0;32m    825\u001b[0m                                             jconf, batchSize)\n\u001b[0;32m    826\u001b[0m \u001b[39mreturn\u001b[39;00m RDD(jrdd, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mE:\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopFile.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: sparkstandalone\r\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)\r\n\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)\r\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:201)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:186)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:530)\r\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:499)\r\n\tat org.apache.spark.SparkContext.$anonfun$newAPIHadoopFile$2(SparkContext.scala:1254)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\r\n\tat org.apache.spark.SparkContext.newAPIHadoopFile(SparkContext.scala:1242)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDDFromClassNames(PythonRDD.scala:399)\r\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopFile(PythonRDD.scala:355)\r\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopFile(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.UnknownHostException: sparkstandalone\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"hdfs://sparkstandalone:8020/data/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 21), ('spark', 7), ('world', 14)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# 同方式一，使用textfiles，从其他文件系统创建\n",
    "# 只不过这个时候需要声明一下文件系统的协议\n",
    "# 如：hdfs://, s3a://,\n",
    "distFile = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# SparkContext.wholeTextFiles可以读取整个文件夹下面的文件\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# 调用textfiles时，并不会加载数据到内存\n",
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# map是一个transformation操作，也不会加载数据到内存\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "# reduce是一个action操作，会记载数据到内存当中，且运算完毕后只会返回一个结果，\n",
    "# 若后续还需要使用这个rdd，则需要调用持久化方法，将其持久化到内存中\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark允许自定义方法用于数据的处理，如下，可对一行进行单词次数统计\n",
    "def wordCount(s:str):\n",
    "    words = s.split(' ')\n",
    "    return len(words)\n",
    "\n",
    "lines.map(wordCount).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello spark', 'hello world', 'hello world']\n"
     ]
    }
   ],
   "source": [
    "# 也可以通过定义对像，在对象的方法中进行处理，\n",
    "# 但是这种方式会造成整个对象会被发送到集群当中\n",
    "# 原因这个方法调用了该类当中的其他方法\n",
    "class MyClass(object):\n",
    "    def func(self,s):\n",
    "        return s\n",
    "    def doStuff(self,rdd):\n",
    "        return rdd.map(self.func)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellohello spark', 'Hellohello world', 'Hellohello world']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似的，若是对象中处理rdd的方式引用了该类当中的其他属性\n",
    "# 也会造成整个对象被发送到集群当中\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hellohello spark', 'Hellohello world', 'Hellohello world']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上面这两种情况都会造成内存的浪费（spark使用内存存储数据）\n",
    "# 最好的方式是定义局部变量赋值，避免对对象中其他属性的直接引用\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        field = self.field # 通过局部变量的方式，避免对类中其他变量的引用\n",
    "        return rdd.map(lambda s: field + s)\n",
    "handler = MyClass()\n",
    "result = handler.doStuff(lines).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello world', 2), ('hello spark', 1)]\n",
      "[('hello world', 2), ('hello spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"hdfs://sparkstandalone:8020/data/data.txt\")\n",
    "# 处理成键值对，键是单词，值为1\n",
    "pairs = lines.map(lambda s:(s,1))\n",
    "# 按单词聚合后累加，注意reduceByKey不是action操作，是transformation操作\n",
    "counts = pairs.reduceByKey(lambda a,b:a+b)\n",
    "# collect是action操作\n",
    "print(counts.collect())\n",
    "# 按单词排序\n",
    "print(counts.sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "broadcastVar = sc.broadcast(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6,7,8,9,0,]\n",
    "broadcastVar = sc.broadcast(b)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.destroy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "type(accum)\n",
    "print(accum)\n",
    "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "# 这里实现了一个向量的加法\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value: list) -> list:\n",
    "        return [0.0]*len(value)\n",
    "    \n",
    "    # 累加器提供一个add方法，这个是对add方法的实现\n",
    "    def addInPlace(self, value1: list, value2: list) -> list:\n",
    "        for i in range(len(value1)):\n",
    "            value1[i] += value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "va = sc.accumulator([1,2,3],VectorAccumulatorParam())\n",
    "print(\"init:\",va.value)\n",
    "data = [[x]*3 for x in range(1,4)]\n",
    "# data = [1,2,3]\n",
    "rdd = sc.parallelize(data)\n",
    "# # 定义一个函数，用于执行这样的累加运算\n",
    "# def g(x):\n",
    "#     global va\n",
    "#     va.add([x]*3) # 如果 data = [1,2,3]，则可以这样子操作\n",
    "# rdd.foreach(g)\n",
    "# print(\"after oper:\",va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd.map(lambda x:va.add(x))\n",
    "# 此时，radd还未执行action，不会修改累加器的值！！！\n",
    "print(va.value)\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0, 8.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "# rdd执行了action，会修改累加器的值！！\n",
    "rdd.map(lambda x:va.add(x)).collect()\n",
    "print(va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return f(x)\n",
    "data.map(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile.map(lambda s:len(s)).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData.reduce(lambda a,b:a+b).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

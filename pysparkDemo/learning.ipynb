{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建连接配置,连接到standalone模式的集群\n",
    "conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"spark://sparkstandalone:7077\")\n",
    "# 设定driver的地址，非常重要，standalone模式的集群\n",
    "conf.set(\"spark.driver.host\",\"192.168.88.1\")\n",
    "# 获取spark上下文,创建到集群的连接\n",
    "sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建连接配置，本地连接\n",
    "# conf = SparkConf().setAppName('sparkRddDemo').setMaster(\"local[2]\")\n",
    "# # 获取spark上下文,创建到集群的连接\n",
    "# sc =  SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有两种方式可以创建rdds，一种是通过上下文提供的并行化方法从一个可迭代对象或者collection中获取\n",
    "# 另一种是内部的存储系统\n",
    "# 下面从一个可迭代的对象中获取\n",
    "data = [i for i in range(1,6)]\n",
    "distData = sc.parallelize(data)\n",
    "print(type(distData))\n",
    "print(distData.reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFile = sc.textFile(\"./*.md\")\n",
    "# 计算所有的单词的长度\n",
    "print(distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建\n",
    "distFiles = sc.wholeTextFiles(\"./\")\n",
    "# distFiles.map(lambda line:len(line)).reduce(lambda a,b:a+b)\n",
    "print(distFiles.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将distFiles使用pickle的方式进行持久化\n",
    "distFile.saveAsPickleFile(\"ts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面通过pickle的方式读取持久化的数据\n",
    "ds = sc.pickleFile(\"ts.pickle\")\n",
    "print(type(ds))\n",
    "ds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和读取sequenceFiles\n",
    "rdd = sc.parallelize(range(1,4)).map(lambda x:(x,\"a\"*x))\n",
    "print(type(rdd))\n",
    "rdd.saveAsSequenceFile('sequence/to/file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sequenceFiles文件\n",
    "sorted(sc.sequenceFile('sequence/to/file').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data.txt\")\n",
    "lineLengths = lines.map(lambda s:len(s))\n",
    "print(lineLengths)\n",
    "totalLengths = lineLengths.reduce(lambda a,b:a+b)\n",
    "print(totalLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"hdfs://sparkstandalone:8020/data/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    "    conf={\n",
    "     \"mapreduce.input.fileinputformat.split.maxsize\": \"4194304\"\n",
    "     # \"mapreduce.input.fileinputformat.split.minsize\":\" 4194304\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 574560),\n",
       " ('yiqiatguigu', 3),\n",
       " ('hello', 21),\n",
       " ('spark', 7),\n",
       " ('atguigu', 1149117),\n",
       " ('world', 14),\n",
       " ('yiqi', 574557)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用hadoop的inputformat来读取数据\n",
    "path = \"hdfs://sparkstandalone:8020/data/\"\n",
    "rdd = sc.newAPIHadoopFile(\n",
    "    path=path,\n",
    "    inputFormatClass=\"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.LongWritable\",\n",
    "    valueClass=\"org.apache.hadoop.io.Text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yiqiatguigu', 3),\n",
       " ('hello', 21),\n",
       " ('spark', 7),\n",
       " ('atguigu', 1149117),\n",
       " ('yiqi', 574557),\n",
       " ('hadoop', 574560),\n",
       " ('world', 14)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv =rdd.flatMap(lambda x:x[1].split(' ')).map(lambda x:(x,1))\n",
    "word_kv.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     words \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(words)\n\u001b[1;32m----> 5\u001b[0m lines\u001b[39m.\u001b[39mmap(wordCount)\u001b[39m.\u001b[39mcollect()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "def wordCount(s:str):\n",
    "    words = s.split(' ')\n",
    "    return len(words)\n",
    "\n",
    "lines.map(wordCount).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def func(self,s):\n",
    "        return s\n",
    "    def doStuff(self,rdd):\n",
    "        return rdd.map(self.func)\n",
    "handler = MyClass()\n",
    "handler.doStuff(lines).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)\n",
    "handler = MyClass()\n",
    "handler.doStuff(lines).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        field = self.field # 通过局部变量的方式，避免对类中其他变量的引用\n",
    "        return rdd.map(lambda s: field + s)\n",
    "handler = MyClass()\n",
    "handler.doStuff(lines).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('data.txt')\n",
    "pairs = lines.map(lambda s:(s,1))\n",
    "counts = pairs.reduceByKey(lambda a,b:a+b)\n",
    "print(counts.collect())\n",
    "print(counts.sortByKey(ascending=False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "broadcastVar = sc.broadcast(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6,7,8,9,0,]\n",
    "broadcastVar = sc.broadcast(b)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.destroy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "type(accum)\n",
    "print(accum)\n",
    "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "\n",
    "    def zero(self, value: list) -> list:\n",
    "        return [0.0]*len(value)\n",
    "    \n",
    "    # 累加器提供一个add方法，这个是对add方法的实现\n",
    "    def addInPlace(self, value1: list, value2: list) -> list:\n",
    "        for i in range(len(value1)):\n",
    "            value1[i] += value2[i]\n",
    "        return value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "va = sc.accumulator([1,2,3],VectorAccumulatorParam())\n",
    "print(\"init:\",va.value)\n",
    "data = [[x]*3 for x in range(1,4)]\n",
    "# data = [1,2,3]\n",
    "rdd = sc.parallelize(data)\n",
    "# # 定义一个函数，用于执行这样的累加运算\n",
    "# def g(x):\n",
    "#     global va\n",
    "#     va.add([x]*3) # 如果 data = [1,2,3]，则可以这样子操作\n",
    "# rdd.foreach(g)\n",
    "# print(\"after oper:\",va.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x:va.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x:va.add(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return f(x)\n",
    "data.map(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile.map(lambda s:len(s)).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData.reduce(lambda a,b:a+b).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
